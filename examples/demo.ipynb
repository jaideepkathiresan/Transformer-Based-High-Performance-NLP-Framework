{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperText-Infinite: Interactive Demo\n",
    "\n",
    "Welcome to the interactive showcase of **HyperText-Infinite**, a high-performance NLP framework built from scratch to demonstrate enterprise-grade optimization techniques.\n",
    "\n",
    "## üéØ Capabilities Shown\n",
    "1. **LLaMA Architecture**: Running `LlamaProto` with RoPE and RMSNorm.\n",
    "2. **Hyper-Inference**: Optimized generation loop.\n",
    "3. **Systems Engineering**: Custom C++ Kernels (falling back to Python if not compiled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from hypertext.models.llama_proto import LlamaProto\n",
    "from hypertext.ops import HAS_C_EXT\n",
    "\n",
    "print(f\"HyperText Backend: {'üöÄ C++ Optimized' if HAS_C_EXT else 'üêç Pure Python Fallback'}\")\n",
    "device = 'cpu' # Use 'cuda' if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Initialization\n",
    "Loading a text-generation model with parameters similar to a small GPT-2/LLaMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "\n",
    "print(\"Initializing LlamaProto...\")\n",
    "model = LlamaProto(vocab_size, d_model, n_layers, n_heads).to(device)\n",
    "print(f\"Model instantiated with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. High-Throughput Generation\n",
    "Running the inference loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = torch.randint(0, vocab_size, (1, 10)).to(device)\n",
    "max_new_tokens = 50\n",
    "\n",
    "print(\"Generating tokens...\")\n",
    "start = time.time()\n",
    "\n",
    "# Simple autoregressive loop\n",
    "ctx = prompt.clone()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(ctx)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        ctx = torch.cat((ctx, next_token), dim=1)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Generated {max_new_tokens} tokens in {end-start:.4f}s\")\n",
    "print(f\"Speed: {max_new_tokens / (end-start):.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Mixture of Experts (MoE) Inspection\n",
    "Demonstrating the sparse gating mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypertext.models.moe import MoELayer\n",
    "\n",
    "moe = MoELayer(d_model=512, d_ff=2048, num_experts=8, k=2)\n",
    "x = torch.randn(4, 10, 512) # (Batch, Seq, Dim)\n",
    "\n",
    "output = moe(x)\n",
    "print(f\"MoE Output Shape: {output.shape}\")\n",
    "print(\"Active Experts: Top-2 per token routed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
